import os
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Constants for URLs
PITCHING_STATS_URL = 'https://razzball.com/mlbpitchingstats-last30days/'
XERA_URL = 'https://baseballsavant.mlb.com/leaderboard/expected_statistics?type=pitcher&year=2024&position=&team=&filterType=bip&min=q&sort=14&sortDir=asc'

# Constants for file paths
DESKTOP_PATH = os.path.expanduser('~/Desktop')

def setup_webdriver():
    """Set up Selenium WebDriver with headless Chrome options."""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    return webdriver.Chrome(options=options)

def scrape_pitcher_stats():
    """Scrape pitcher stats from the given URL and save the data to a CSV file."""
    try:
        driver = setup_webdriver()
        driver.get(PITCHING_STATS_URL)

        # Wait for the table to be present on the page
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, 'neorazzstatstable'))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        table = soup.find('table', {'id': 'neorazzstatstable'})
        pitcher_data = []

        # Extract data from the table
        for row in table.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) > 0:
                pitcher_name = cells[1].text.strip()  # 2nd column
                fip = cells[19].text.strip()  # Corrected index for 20th column
                whip = cells[20].text.strip()  # Corrected index for 21st column
                pitcher_data.append({'pitcher_name': pitcher_name, 'fip': fip, 'whip': whip})

        df = pd.DataFrame(pitcher_data)
        save_to_csv(df, 'pitcher_era.csv')
        return df, pitcher_data
    except Exception as e:
        print(f"An error occurred while scraping pitcher stats: {e}")
        return None, None
    finally:
        driver.quit()

def scrape_xera():
    """Scrape xERA stats from the given URL and save the data to a CSV file."""
    try:
        driver = setup_webdriver()
        driver.get(XERA_URL)

        # Wait for the expected stats div to be present on the page
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.ID, 'expected_stats'))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        div = soup.find('div', {'id': 'expected_stats'})
        if div is None:
            raise ValueError("Div not found on the page.")

        table = div.find('table')
        if table is None:
            raise ValueError("Table not found within the div.")

        pitcher_xera = []
        for row in table.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) > 0:
                name_parts = cells[1].text.strip().split(", ")
                pitcher_name = f"{name_parts[1]} {name_parts[0]}" if len(name_parts) == 2 else cells[1].text.strip()
                xera = cells[15].text.strip()  # 16th column
                pitcher_xera.append({'pitcher_name': pitcher_name, 'xera': xera})

        df = pd.DataFrame(pitcher_xera)
        save_to_csv(df, 'pitcher_xera.csv')
        return df, pitcher_xera
    except Exception as e:
        print(f"An error occurred while scraping xERA stats: {e}")
        return None, None
    finally:
        driver.quit()

def save_to_csv(df, filename):
    """Save a DataFrame to a CSV file on the desktop."""
    save_path = os.path.join(DESKTOP_PATH, filename)
    df.to_csv(save_path, index=False)
    print(f"Data saved to {save_path}")

if __name__ == "__main__":
    df_pitcher, pitcher_era_list = scrape_pitcher_stats()
    if pitcher_era_list is not None:
        print("Pitcher ERA Data:")
        for pitcher_era in pitcher_era_list:
            print(pitcher_era)

    df_xera, pitcher_xera_list = scrape_xera()
    if pitcher_xera_list is not None:
        print("Pitcher xERA Data:")
        for pitcher_xera in pitcher_xera_list:
            print(pitcher_xera)
