import os
import pandas as pd
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager import WebDriverManager 

class FangraphsScraper:
    """
    Scrapes pitcher and team statistics from Fangraphs and save the data to TSV files. 
    """
    URLS = {
        'pitcher_stats_home': "https://www.fangraphs.com/leaders/splits-leaderboards?splitArr=9...",
        'pitcher_stats_away': "https://www.fangraphs.com/leaders/splits-leaderboards?splitArr=10...",
        'team_stats_home_vs_LHP': "https://www.fangraphs.com/leaders/splits-leaderboards?splitArr=1,7...",
        'team_stats_home_vs_RHP': "https://www.fangraphs.com/leaders/splits-leaderboards?splitArr=2,7...",
        'team_stats_away_vs_LHP': "https://www.fangraphs.com/leaders/splits-leaderboards?splitArr=1,8...",
        'team_stats_away_vs_RHP': "https://www.fangraphs.com/leaders/splits-leaderboards?splitArr=2,8..."
    }

    HEADERS = {
        'pitcher_headers': ['Name', 'Tm', 'K%', 'BB%', 'WHIP', 'FIP'],
        'team_headers': ['Tm', 'BB%', 'K%', 'SLG', 'OPS', 'wRC+']
    }

    def __init__(self, driver_manager):
        self.driver_manager = driver_manager
        self.driver = self.driver_manager.get_driver()

    def scrape_fangraphs_stats(self, url, target_headers):
        self.driver.get(url)
        WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, 'data-grid-wrapper'))
        )

        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        table = soup.find('div', class_='table-scroll')

        headers = [th.text.strip() for th in table.find('thead').find_all('th')]
        indices = [i for i, h in enumerate(headers) if h in target_headers]
        rows = table.find('tbody').find_all('tr')

        data_rows = []
        for row in rows:
            columns = [td.text.strip() for td in row.find_all('td')]
            filtered_columns = [columns[i] for i in indices]
            data_rows.append(filtered_columns)

        df = pd.DataFrame(data_rows, columns=[headers[i] for i in indices])

        return df

    def save_dataframes_to_tsv(self, data_frames, folder_name="TSV Files"):
        project_path = os.path.dirname(os.path.abspath(__file__))

        folder_path = os.path.join(project_path, folder_name)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        for category, df in data_frames.items():
            tsv_filename = f'{category}.tsv'
            save_path = os.path.join(folder_path, tsv_filename)
            df.to_csv(save_path, sep='\t', index=False)
            print(f"Saved {category} data to {save_path}")

    def close(self):
        self.driver_manager.close_driver()


def main():
    driver_manager = WebDriverManager(headless=True)  
    scraper = FangraphsScraper(driver_manager)

    data_frames = {}
    for category, url in scraper.URLS.items():
        if "pitcher" in category:
            df = scraper.scrape_fangraphs_stats(url, scraper.HEADERS['pitcher_headers'])
        else:
            df = scraper.scrape_fangraphs_stats(url, scraper.HEADERS['team_headers'])
        data_frames[category] = df

    scraper.save_dataframes_to_tsv(data_frames)
    scraper.close()

if __name__ == "__main__":
    main()
